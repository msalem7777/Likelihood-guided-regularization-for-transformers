{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5427cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/My files/NPM/repo/Likelihood-guided-regularization-for-transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25e7f4a-cc4d-4c32-b988-52350b6950d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from main.pVisionTransformer_Trainer import pVisionTransformerTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1706d4-b6b6-4297-a350-174cfee45d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run data_loader/mnist_fmnist_downloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49863b4-afde-4e04-a092-6e3744ce54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    use_gpu=torch.cuda.is_available(),  # Automatically detect if a GPU is available\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    gpu=0,  # GPU index to use (set to 0 if only one GPU is available)\n",
    "    use_multi_gpu=False,  # Set to True if you want to use multiple GPUs\n",
    "    device_ids=[0],  # List of GPU device IDs (if using multiple GPUs)\n",
    "    num_models=1,  # Number of models to initialize\n",
    "    img_size = 28,\n",
    "    patch_size=7,  # Patch size for the Vision Transformer\n",
    "    num_classes=10,  # Number of output classes (10 for MNIST)\n",
    "    embed_dim=64,  # Embedding dimension for the Vision Transformer\n",
    "    num_heads=32,  # Number of attention heads\n",
    "    depth=2,  # Number of Transformer blocks\n",
    "    dropout=0.0,  # Dropout rate\n",
    "    path = '.',\n",
    "    root_path = '.',\n",
    "    checkpoints = './checkpoints',\n",
    "    dataset = 'mnist',\n",
    "    data_path = './mnist',\n",
    "    batch_size=20,\n",
    "    lradj = \"type2\",\n",
    "    val_split=0.85,  # Fraction of data for validation\n",
    "    test_split=0.10,  # Fraction of data for testing\n",
    "    patience = 100,\n",
    "    lambda_weight1 = 0.000001,\n",
    "    lambda_weight2 = 0.000001,\n",
    "    learning_rate = 0.001,\n",
    "    kl_pen = 0.000001,\n",
    "    train_epochs = 1, \n",
    "    ising_epochs = 1, \n",
    "    addtl_ft = 5,\n",
    "    ising_type = \"diag_saliency_scores\", # options are: \"diag_saliency_scores\", \"LM_saliency_scores\", \"no_saliency_scores\",\n",
    "    num_workers = 0, # number of workers for data loading\n",
    "    ising_batch = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c7ee0f-c2ff-4a13-bbb7-114ae89de92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "trainer = pVisionTransformerTrainer(args) # instantiated class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54bc69d7-edff-4c12-9a49-3746d60017f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformerWithBBB(\n",
       "  (patch_embedding): BBBLinear(in_features=147, out_features=64, bias=True)\n",
       "  (encoder): ModuleList(\n",
       "    (0-1): 2 x TransformerEncoderLayerWithBBB(\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): BBBLinear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): BBBLinear(in_features=256, out_features=64, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classification_head): Sequential(\n",
       "    (0): BBBLinear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "    (3): BBBLinear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_model = trainer.models[0] # checking model instantiated properly\n",
    "active_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89443b80-c4b4-46cc-85c1-c077a191d718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training length: 8100\n",
      "Current phase set to: pilot\n",
      "\titers: 100, epoch: 1 | loss for each model: [2.4018735492229464]\n",
      "\tspeed: 0.0134s/iter\n",
      "\titers: 200, epoch: 1 | loss for each model: [2.366633540391922]\n",
      "\tspeed: 0.0113s/iter\n",
      "\titers: 300, epoch: 1 | loss for each model: [2.366356418132782]\n",
      "\tspeed: 0.0111s/iter\n",
      "\titers: 400, epoch: 1 | loss for each model: [2.357648542523384]\n",
      "\tspeed: 0.0105s/iter\n",
      "\titers: 500, epoch: 1 | loss for each model: [2.3534741265773773]\n",
      "\tspeed: 0.0104s/iter\n",
      "\titers: 600, epoch: 1 | loss for each model: [2.350183275938034]\n",
      "\tspeed: 0.0103s/iter\n",
      "\titers: 700, epoch: 1 | loss for each model: [2.331059169939586]\n",
      "\tspeed: 0.0104s/iter\n",
      "\titers: 800, epoch: 1 | loss for each model: [2.3194118189718576]\n",
      "\tspeed: 0.0105s/iter\n",
      "\titers: 900, epoch: 1 | loss for each model: [2.2986377289808457]\n",
      "\tspeed: 0.0105s/iter\n",
      "\titers: 1000, epoch: 1 | loss for each model: [2.2769559001624584]\n",
      "\tspeed: 0.0105s/iter\n",
      "\titers: 1100, epoch: 1 | loss for each model: [2.259451456720179]\n",
      "\tspeed: 0.0108s/iter\n",
      "\titers: 1200, epoch: 1 | loss for each model: [2.230533362242083]\n",
      "\tspeed: 0.0107s/iter\n",
      "\titers: 1300, epoch: 1 | loss for each model: [2.1961534551531074]\n",
      "\tspeed: 0.0105s/iter\n",
      "\titers: 1400, epoch: 1 | loss for each model: [2.183462010813611]\n",
      "\tspeed: 0.0105s/iter\n",
      "\titers: 1500, epoch: 1 | loss for each model: [2.1581186989645165]\n",
      "\tspeed: 0.0107s/iter\n",
      "\titers: 1600, epoch: 1 | loss for each model: [2.124850149396807]\n",
      "\tspeed: 0.0107s/iter\n",
      "\titers: 1700, epoch: 1 | loss for each model: [2.102262796038652]\n",
      "\tspeed: 0.0105s/iter\n",
      "\titers: 1800, epoch: 1 | loss for each model: [2.082606981460833]\n",
      "\tspeed: 0.0105s/iter\n",
      "\titers: 1900, epoch: 1 | loss for each model: [2.0640031872483853]\n",
      "\tspeed: 0.0107s/iter\n",
      "\titers: 2000, epoch: 1 | loss for each model: [2.0437444093553347]\n",
      "\tspeed: 0.0111s/iter\n",
      "\titers: 2100, epoch: 1 | loss for each model: [2.0256668455073874]\n",
      "\tspeed: 0.0107s/iter\n",
      "\titers: 2200, epoch: 1 | loss for each model: [1.9967254098123786]\n",
      "\tspeed: 0.0105s/iter\n",
      "\titers: 2300, epoch: 1 | loss for each model: [1.9770118070325977]\n",
      "\tspeed: 0.0109s/iter\n",
      "\titers: 2400, epoch: 1 | loss for each model: [1.9589724721214345]\n",
      "\tspeed: 0.0107s/iter\n",
      "\titers: 2500, epoch: 1 | loss for each model: [1.9392152865315322]\n",
      "\tspeed: 0.0107s/iter\n",
      "\titers: 2600, epoch: 1 | loss for each model: [1.9126719501827252]\n",
      "\tspeed: 0.0108s/iter\n",
      "\titers: 2700, epoch: 1 | loss for each model: [1.8885941287631565]\n",
      "\tspeed: 0.0105s/iter\n",
      "\titers: 2800, epoch: 1 | loss for each model: [1.8728343342869527]\n",
      "\tspeed: 0.0105s/iter\n",
      "\titers: 2900, epoch: 1 | loss for each model: [1.8481094147142119]\n",
      "\tspeed: 0.0106s/iter\n",
      "\titers: 3000, epoch: 1 | loss for each model: [1.8262515133033448]\n",
      "\tspeed: 0.0108s/iter\n",
      "\titers: 3100, epoch: 1 | loss for each model: [1.8153876354136211]\n",
      "\tspeed: 0.0108s/iter\n",
      "\titers: 3200, epoch: 1 | loss for each model: [1.7963266211516247]\n",
      "\tspeed: 0.0108s/iter\n",
      "\titers: 3300, epoch: 1 | loss for each model: [1.7808727065198235]\n",
      "\tspeed: 0.0108s/iter\n",
      "\titers: 3400, epoch: 1 | loss for each model: [1.7630926602499248]\n",
      "\tspeed: 0.0106s/iter\n",
      "\titers: 3500, epoch: 1 | loss for each model: [1.7481038772732966]\n",
      "\tspeed: 0.0107s/iter\n",
      "\titers: 3600, epoch: 1 | loss for each model: [1.733956416011408]\n",
      "\tspeed: 0.0114s/iter\n",
      "\titers: 3700, epoch: 1 | loss for each model: [1.7140926049556877]\n",
      "\tspeed: 0.0100s/iter\n",
      "\titers: 3800, epoch: 1 | loss for each model: [1.693396089753773]\n",
      "\tspeed: 0.0099s/iter\n",
      "\titers: 3900, epoch: 1 | loss for each model: [1.6834408881030731]\n",
      "\tspeed: 0.0104s/iter\n",
      "\titers: 4000, epoch: 1 | loss for each model: [1.670678751525512]\n",
      "\tspeed: 0.0107s/iter\n",
      "\titers: 4100, epoch: 1 | loss for each model: [1.6537635253354466]\n",
      "\tspeed: 0.0104s/iter\n",
      "\titers: 4200, epoch: 1 | loss for each model: [1.6412207839600013]\n",
      "\tspeed: 0.0160s/iter\n",
      "\titers: 4300, epoch: 1 | loss for each model: [1.6269203255859164]\n",
      "\tspeed: 0.0141s/iter\n",
      "\titers: 4400, epoch: 1 | loss for each model: [1.6182110484303442]\n",
      "\tspeed: 0.0121s/iter\n",
      "\titers: 4500, epoch: 1 | loss for each model: [1.60545312931384]\n",
      "\tspeed: 0.0119s/iter\n",
      "\titers: 4600, epoch: 1 | loss for each model: [1.5992993000490543]\n",
      "\tspeed: 0.0122s/iter\n",
      "\titers: 4700, epoch: 1 | loss for each model: [1.589552511403285]\n",
      "\tspeed: 0.0124s/iter\n",
      "\titers: 4800, epoch: 1 | loss for each model: [1.580085647328468]\n",
      "\tspeed: 0.0192s/iter\n",
      "\titers: 4900, epoch: 1 | loss for each model: [1.5706063555052003]\n",
      "\tspeed: 0.0159s/iter\n",
      "\titers: 5000, epoch: 1 | loss for each model: [1.5636378891039684]\n",
      "\tspeed: 0.0124s/iter\n",
      "\titers: 5100, epoch: 1 | loss for each model: [1.5516711479673369]\n",
      "\tspeed: 0.0144s/iter\n",
      "\titers: 5200, epoch: 1 | loss for each model: [1.5418440138468228]\n",
      "\tspeed: 0.0126s/iter\n",
      "\titers: 5300, epoch: 1 | loss for each model: [1.5307824998596156]\n",
      "\tspeed: 0.0120s/iter\n",
      "\titers: 5400, epoch: 1 | loss for each model: [1.5236204854646336]\n",
      "\tspeed: 0.0132s/iter\n",
      "\titers: 5500, epoch: 1 | loss for each model: [1.5124079619719506]\n",
      "\tspeed: 0.0123s/iter\n",
      "\titers: 5600, epoch: 1 | loss for each model: [1.5044469845345045]\n",
      "\tspeed: 0.0119s/iter\n",
      "\titers: 5700, epoch: 1 | loss for each model: [1.4947484409402687]\n",
      "\tspeed: 0.0118s/iter\n",
      "\titers: 5800, epoch: 1 | loss for each model: [1.4844946862998067]\n",
      "\tspeed: 0.0124s/iter\n",
      "\titers: 5900, epoch: 1 | loss for each model: [1.477959710285137]\n",
      "\tspeed: 0.0124s/iter\n",
      "\titers: 6000, epoch: 1 | loss for each model: [1.4694675733108504]\n",
      "\tspeed: 0.0119s/iter\n",
      "\titers: 6100, epoch: 1 | loss for each model: [1.463871108788442]\n",
      "\tspeed: 0.0121s/iter\n",
      "\titers: 6200, epoch: 1 | loss for each model: [1.4520608564195199]\n",
      "\tspeed: 0.0119s/iter\n",
      "\titers: 6300, epoch: 1 | loss for each model: [1.4445622467212584]\n",
      "\tspeed: 0.0122s/iter\n",
      "\titers: 6400, epoch: 1 | loss for each model: [1.439044655391988]\n",
      "\tspeed: 0.0129s/iter\n",
      "\titers: 6500, epoch: 1 | loss for each model: [1.4302971965630078]\n",
      "\tspeed: 0.0125s/iter\n",
      "\titers: 6600, epoch: 1 | loss for each model: [1.4225859626544715]\n",
      "\tspeed: 0.0136s/iter\n",
      "\titers: 6700, epoch: 1 | loss for each model: [1.419442093372014]\n",
      "\tspeed: 0.0137s/iter\n",
      "\titers: 6800, epoch: 1 | loss for each model: [1.4114527434985247]\n",
      "\tspeed: 0.0123s/iter\n",
      "\titers: 6900, epoch: 1 | loss for each model: [1.4034803342038453]\n",
      "\tspeed: 0.0122s/iter\n",
      "\titers: 7000, epoch: 1 | loss for each model: [1.3958231010064839]\n",
      "\tspeed: 0.0120s/iter\n",
      "\titers: 7100, epoch: 1 | loss for each model: [1.3907153994717305]\n",
      "\tspeed: 0.0122s/iter\n",
      "\titers: 7200, epoch: 1 | loss for each model: [1.3831477813747715]\n",
      "\tspeed: 0.0124s/iter\n",
      "\titers: 7300, epoch: 1 | loss for each model: [1.3791280684686544]\n",
      "\tspeed: 0.0126s/iter\n",
      "\titers: 7400, epoch: 1 | loss for each model: [1.3725181286927608]\n",
      "\tspeed: 0.0123s/iter\n",
      "\titers: 7500, epoch: 1 | loss for each model: [1.3657338694613894]\n",
      "\tspeed: 0.0125s/iter\n",
      "\titers: 7600, epoch: 1 | loss for each model: [1.3621146078667108]\n",
      "\tspeed: 0.0125s/iter\n",
      "\titers: 7700, epoch: 1 | loss for each model: [1.3525311133280153]\n",
      "\tspeed: 0.0131s/iter\n",
      "\titers: 7800, epoch: 1 | loss for each model: [1.345455954272044]\n",
      "\tspeed: 0.0123s/iter\n",
      "\titers: 7900, epoch: 1 | loss for each model: [1.3392713515610195]\n",
      "\tspeed: 0.0140s/iter\n",
      "\titers: 8000, epoch: 1 | loss for each model: [1.3321448987165152]\n",
      "\tspeed: 0.0147s/iter\n",
      "\titers: 8100, epoch: 1 | loss for each model: [1.3298402719450877]\n",
      "\tspeed: 3.2478s/iter\n",
      "Epoch: 1 cost time: 419.1755540370941\n",
      "Epoch: 1, Train Loss: [1.3298402719450877], Vali Loss: [0.8188599679205153], Test Loss: [0.814953605333964]\n",
      "🔁 Phase changed from None → pilot\n",
      "Model 0: Validation loss decreased (inf --> 0.818860).  Saving model ...\n",
      "Current phase set to: ising\n",
      "\titers: 100, epoch: 2 | loss for each model: [1.3002330073714257]\n",
      "\tspeed: 0.0139s/iter\n",
      "\titers: 200, epoch: 2 | loss for each model: [1.1987651385366918]\n",
      "\tspeed: 0.0145s/iter\n",
      "\titers: 300, epoch: 2 | loss for each model: [1.104222468060131]\n",
      "\tspeed: 0.0135s/iter\n",
      "\titers: 400, epoch: 2 | loss for each model: [0.9898383272445063]\n",
      "\tspeed: 0.0133s/iter\n",
      "\titers: 500, epoch: 2 | loss for each model: [0.9711571358679794]\n",
      "\tspeed: 0.0137s/iter\n",
      "\titers: 600, epoch: 2 | loss for each model: [0.8866717507241992]\n",
      "\tspeed: 0.0134s/iter\n",
      "\titers: 700, epoch: 2 | loss for each model: [0.8690442683010562]\n",
      "\tspeed: 0.0135s/iter\n",
      "\titers: 800, epoch: 2 | loss for each model: [0.8441050491709029]\n",
      "\tspeed: 0.0133s/iter\n",
      "\titers: 900, epoch: 2 | loss for each model: [0.8033778835869614]\n",
      "\tspeed: 0.0133s/iter\n",
      "\titers: 1000, epoch: 2 | loss for each model: [0.7462732546875486]\n",
      "\tspeed: 0.0133s/iter\n",
      "\titers: 1100, epoch: 2 | loss for each model: [0.6896556251625667]\n",
      "\tspeed: 0.0133s/iter\n",
      "\titers: 1200, epoch: 2 | loss for each model: [0.6494912225191408]\n",
      "\tspeed: 0.0136s/iter\n",
      "\titers: 1300, epoch: 2 | loss for each model: [0.6082445586954343]\n",
      "\tspeed: 0.0132s/iter\n",
      "\titers: 1400, epoch: 2 | loss for each model: [0.5669732125832894]\n",
      "\tspeed: 0.0135s/iter\n",
      "\titers: 1500, epoch: 2 | loss for each model: [0.5326089558871796]\n",
      "\tspeed: 0.0135s/iter\n",
      "\titers: 1600, epoch: 2 | loss for each model: [0.5004845808149843]\n",
      "\tspeed: 0.0137s/iter\n",
      "\titers: 1700, epoch: 2 | loss for each model: [0.47135793612488375]\n",
      "\tspeed: 0.0140s/iter\n",
      "\titers: 1800, epoch: 2 | loss for each model: [0.44528949171620846]\n",
      "\tspeed: 0.0137s/iter\n",
      "\titers: 1900, epoch: 2 | loss for each model: [0.42232343762608493]\n",
      "\tspeed: 0.0136s/iter\n",
      "\titers: 2000, epoch: 2 | loss for each model: [0.4014612189611057]\n",
      "\tspeed: 0.0149s/iter\n",
      "\titers: 2100, epoch: 2 | loss for each model: [0.38250839486149113]\n",
      "\tspeed: 0.0147s/iter\n",
      "\titers: 2200, epoch: 2 | loss for each model: [0.3654402681747282]\n",
      "\tspeed: 0.0149s/iter\n",
      "\titers: 2300, epoch: 2 | loss for each model: [0.34968695318737425]\n",
      "\tspeed: 0.0140s/iter\n",
      "\titers: 2400, epoch: 2 | loss for each model: [0.33529951065279845]\n",
      "\tspeed: 0.0140s/iter\n",
      "\titers: 2500, epoch: 2 | loss for each model: [0.3219405625874806]\n",
      "\tspeed: 0.0137s/iter\n",
      "\titers: 2600, epoch: 2 | loss for each model: [0.30960928528880677]\n",
      "\tspeed: 0.0139s/iter\n",
      "\titers: 2700, epoch: 2 | loss for each model: [0.29819533947917964]\n",
      "\tspeed: 0.0137s/iter\n",
      "\titers: 2800, epoch: 2 | loss for each model: [0.2875692077167099]\n",
      "\tspeed: 0.0140s/iter\n",
      "\titers: 2900, epoch: 2 | loss for each model: [0.27779335389753224]\n",
      "\tspeed: 0.0141s/iter\n",
      "\titers: 3000, epoch: 2 | loss for each model: [0.26885180464658454]\n",
      "\tspeed: 0.0134s/iter\n",
      "\titers: 3100, epoch: 2 | loss for each model: [0.26125574975521465]\n",
      "\tspeed: 0.0143s/iter\n",
      "\titers: 3200, epoch: 2 | loss for each model: [0.2532227027269899]\n",
      "\tspeed: 0.0159s/iter\n",
      "\titers: 3300, epoch: 2 | loss for each model: [0.2455569832831171]\n",
      "\tspeed: 0.0155s/iter\n",
      "\titers: 3400, epoch: 2 | loss for each model: [0.2383489598071166]\n",
      "\tspeed: 0.0138s/iter\n",
      "\titers: 3500, epoch: 2 | loss for each model: [0.23209410705686268]\n",
      "\tspeed: 0.0144s/iter\n",
      "\titers: 3600, epoch: 2 | loss for each model: [0.2256508171410897]\n",
      "\tspeed: 0.0135s/iter\n",
      "\titers: 3700, epoch: 2 | loss for each model: [0.21956010402255752]\n",
      "\tspeed: 0.0137s/iter\n",
      "\titers: 3800, epoch: 2 | loss for each model: [0.2137967836193711]\n",
      "\tspeed: 0.0139s/iter\n",
      "\titers: 3900, epoch: 2 | loss for each model: [0.20839000780798866]\n",
      "\tspeed: 0.0138s/iter\n",
      "\titers: 4000, epoch: 2 | loss for each model: [0.2031827862501615]\n",
      "\tspeed: 0.0184s/iter\n",
      "\titers: 4100, epoch: 2 | loss for each model: [0.19822857086448356]\n",
      "\tspeed: 0.0192s/iter\n",
      "\titers: 4200, epoch: 2 | loss for each model: [0.1935095651091391]\n",
      "\tspeed: 0.0175s/iter\n",
      "\titers: 4300, epoch: 2 | loss for each model: [0.189009408948803]\n",
      "\tspeed: 0.0210s/iter\n",
      "\titers: 4400, epoch: 2 | loss for each model: [0.1847144903872342]\n",
      "\tspeed: 0.0171s/iter\n",
      "\titers: 4500, epoch: 2 | loss for each model: [0.18062372029257437]\n",
      "\tspeed: 0.0182s/iter\n",
      "\titers: 4600, epoch: 2 | loss for each model: [0.17670940215563896]\n",
      "\tspeed: 0.0170s/iter\n",
      "\titers: 4700, epoch: 2 | loss for each model: [0.1729588848842272]\n",
      "\tspeed: 0.0193s/iter\n",
      "\titers: 4800, epoch: 2 | loss for each model: [0.16935594140528454]\n",
      "\tspeed: 0.0167s/iter\n",
      "\titers: 4900, epoch: 2 | loss for each model: [0.165900131953866]\n",
      "\tspeed: 0.0166s/iter\n",
      "\titers: 5000, epoch: 2 | loss for each model: [0.16488004001852588]\n",
      "\tspeed: 0.0164s/iter\n",
      "\titers: 5100, epoch: 2 | loss for each model: [0.16307188752310406]\n",
      "\tspeed: 0.0171s/iter\n",
      "\titers: 5200, epoch: 2 | loss for each model: [0.16027932663635847]\n",
      "\tspeed: 0.0170s/iter\n",
      "\titers: 5300, epoch: 2 | loss for each model: [0.1573671652044509]\n",
      "\tspeed: 0.0190s/iter\n",
      "\titers: 5400, epoch: 2 | loss for each model: [0.1544555584175813]\n",
      "\tspeed: 0.0199s/iter\n",
      "\titers: 5500, epoch: 2 | loss for each model: [0.1516479840825153]\n",
      "\tspeed: 0.0194s/iter\n",
      "\titers: 5600, epoch: 2 | loss for each model: [0.14894009616237092]\n",
      "\tspeed: 0.0189s/iter\n",
      "\titers: 5700, epoch: 2 | loss for each model: [0.14640451528291065]\n",
      "\tspeed: 0.0329s/iter\n",
      "\titers: 5800, epoch: 2 | loss for each model: [0.14388036664117113]\n",
      "\tspeed: 0.0206s/iter\n",
      "\titers: 5900, epoch: 2 | loss for each model: [0.1414417318761523]\n",
      "\tspeed: 0.0188s/iter\n",
      "\titers: 6000, epoch: 2 | loss for each model: [0.13908543079429495]\n",
      "\tspeed: 0.0182s/iter\n",
      "\titers: 6100, epoch: 2 | loss for each model: [0.1368053643162615]\n",
      "\tspeed: 0.0174s/iter\n",
      "\titers: 6200, epoch: 2 | loss for each model: [0.13459902436538093]\n",
      "\tspeed: 0.0240s/iter\n",
      "\titers: 6300, epoch: 2 | loss for each model: [0.13246265286366113]\n",
      "\tspeed: 0.0169s/iter\n",
      "\titers: 6400, epoch: 2 | loss for each model: [0.13039292788008056]\n",
      "\tspeed: 0.0167s/iter\n",
      "\titers: 6500, epoch: 2 | loss for each model: [0.12838762576923693]\n",
      "\tspeed: 0.0167s/iter\n",
      "\titers: 6600, epoch: 2 | loss for each model: [0.12644277744013116]\n",
      "\tspeed: 0.0168s/iter\n",
      "\titers: 6700, epoch: 2 | loss for each model: [0.12455558196606924]\n",
      "\tspeed: 0.0169s/iter\n",
      "\titers: 6800, epoch: 2 | loss for each model: [0.1227247412354272]\n",
      "\tspeed: 0.0180s/iter\n",
      "\titers: 6900, epoch: 2 | loss for each model: [0.12094613349325788]\n",
      "\tspeed: 0.0173s/iter\n",
      "\titers: 7000, epoch: 2 | loss for each model: [0.11921842877415993]\n",
      "\tspeed: 0.0180s/iter\n",
      "\titers: 7100, epoch: 2 | loss for each model: [0.11754347659734683]\n",
      "\tspeed: 0.0191s/iter\n",
      "\titers: 7200, epoch: 2 | loss for each model: [0.11591093044710109]\n",
      "\tspeed: 0.0168s/iter\n",
      "\titers: 7300, epoch: 2 | loss for each model: [0.11432313668687356]\n",
      "\tspeed: 0.0167s/iter\n",
      "\titers: 7400, epoch: 2 | loss for each model: [0.11277823764996618]\n",
      "\tspeed: 0.0166s/iter\n",
      "\titers: 7500, epoch: 2 | loss for each model: [0.11127453161340052]\n",
      "\tspeed: 0.0167s/iter\n",
      "\titers: 7600, epoch: 2 | loss for each model: [0.1098104085365154]\n",
      "\tspeed: 0.0208s/iter\n",
      "\titers: 7700, epoch: 2 | loss for each model: [0.10838430288003456]\n",
      "\tspeed: 0.0170s/iter\n",
      "\titers: 7800, epoch: 2 | loss for each model: [0.10699476635830389]\n",
      "\tspeed: 0.0169s/iter\n",
      "\titers: 7900, epoch: 2 | loss for each model: [0.10564109677472999]\n",
      "\tspeed: 0.0193s/iter\n",
      "\titers: 8000, epoch: 2 | loss for each model: [0.1043205872224343]\n",
      "\tspeed: 0.0187s/iter\n",
      "Ising hard-threshold dropped params: 36846 (47.66% of 77312)\n",
      "Total model parameters: 189908\n",
      "\titers: 8100, epoch: 2 | loss for each model: [0.10303267897368182]\n",
      "\tspeed: 3.4137s/iter\n",
      "Epoch: 2 cost time: 471.2269170284271\n",
      "Epoch: 2, Train Loss: [0.10303267897368182], Vali Loss: [37.0742914835612], Test Loss: [36.57510312398275]\n",
      "🔁 Phase changed from pilot → ising\n",
      "Saving model 0 during 'ising' phase regardless of loss.\n",
      "Model 0: Validation loss decreased (0.818860 --> 37.074291).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Current phase set to: fine-tuning\n",
      "\titers: 100, epoch: 3 | loss for each model: [2.3889468055963516]\n",
      "\tspeed: 0.0258s/iter\n",
      "\titers: 200, epoch: 3 | loss for each model: [1.6637654112279414]\n",
      "\tspeed: 0.0167s/iter\n",
      "\titers: 300, epoch: 3 | loss for each model: [1.365003758519888]\n",
      "\tspeed: 0.0163s/iter\n",
      "\titers: 400, epoch: 3 | loss for each model: [1.1917816669121384]\n",
      "\tspeed: 0.0221s/iter\n",
      "Epoch: 3 cost time: 8.187917470932007\n",
      "Epoch: 3, Train Loss: [1.18335720812097], Vali Loss: [0.6986400776439243], Test Loss: [0.6816522578398386]\n",
      "🔁 Phase changed from ising → fine-tuning\n",
      "🔄 Resetting early stopping history for fine-tuning phase...\n",
      "Model 0: Validation loss decreased (inf --> 0.698640).  Saving model ...\n",
      "Current phase set to: fine-tuning\n",
      "\titers: 100, epoch: 4 | loss for each model: [0.5811105392873287]\n",
      "\tspeed: 0.0133s/iter\n",
      "\titers: 200, epoch: 4 | loss for each model: [0.5863186146318913]\n",
      "\tspeed: 0.0132s/iter\n",
      "\titers: 300, epoch: 4 | loss for each model: [0.5678948857883612]\n",
      "\tspeed: 0.0135s/iter\n",
      "\titers: 400, epoch: 4 | loss for each model: [0.5678855638951064]\n",
      "\tspeed: 0.0132s/iter\n",
      "Epoch: 4 cost time: 5.392091989517212\n",
      "Epoch: 4, Train Loss: [0.5664700199424485], Vali Loss: [0.5445442537466685], Test Loss: [0.520514244834582]\n",
      "Model 0: Validation loss decreased (0.698640 --> 0.544544).  Saving model ...\n",
      "Current phase set to: fine-tuning\n",
      "\titers: 100, epoch: 5 | loss for each model: [0.44264391154050825]\n",
      "\tspeed: 0.0181s/iter\n",
      "\titers: 200, epoch: 5 | loss for each model: [0.45093467030674217]\n",
      "\tspeed: 0.0165s/iter\n",
      "\titers: 300, epoch: 5 | loss for each model: [0.45986765701323745]\n",
      "\tspeed: 0.0163s/iter\n",
      "\titers: 400, epoch: 5 | loss for each model: [0.45160116924904287]\n",
      "\tspeed: 0.0176s/iter\n",
      "Epoch: 5 cost time: 6.92838191986084\n",
      "Epoch: 5, Train Loss: [0.4516276342541347], Vali Loss: [0.48315354784329734], Test Loss: [0.4580184469620387]\n",
      "Model 0: Validation loss decreased (0.544544 --> 0.483154).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "Current phase set to: fine-tuning\n",
      "\titers: 100, epoch: 6 | loss for each model: [0.3436477730050683]\n",
      "\tspeed: 0.0173s/iter\n",
      "\titers: 200, epoch: 6 | loss for each model: [0.35058159654960036]\n",
      "\tspeed: 0.0169s/iter\n",
      "\titers: 300, epoch: 6 | loss for each model: [0.3449228662252426]\n",
      "\tspeed: 0.0163s/iter\n",
      "\titers: 400, epoch: 6 | loss for each model: [0.3446043514087796]\n",
      "\tspeed: 0.0165s/iter\n",
      "Epoch: 6 cost time: 6.778202533721924\n",
      "Epoch: 6, Train Loss: [0.3442175361201351], Vali Loss: [0.45219373570548166], Test Loss: [0.42062386870384216]\n",
      "Model 0: Validation loss decreased (0.483154 --> 0.452194).  Saving model ...\n",
      "Current phase set to: fine-tuning\n",
      "\titers: 100, epoch: 7 | loss for each model: [0.272071778960526]\n",
      "\tspeed: 0.0167s/iter\n",
      "\titers: 200, epoch: 7 | loss for each model: [0.28369896437972786]\n",
      "\tspeed: 0.0218s/iter\n",
      "\titers: 300, epoch: 7 | loss for each model: [0.28253829228381316]\n",
      "\tspeed: 0.0309s/iter\n",
      "\titers: 400, epoch: 7 | loss for each model: [0.2938844467420131]\n",
      "\tspeed: 0.0199s/iter\n",
      "Epoch: 7 cost time: 9.023903369903564\n",
      "Epoch: 7, Train Loss: [0.29488151935331613], Vali Loss: [0.4344054387675391], Test Loss: [0.4117753009001414]\n",
      "Model 0: Validation loss decreased (0.452194 --> 0.434405).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\My files\\NPM\\repo\\Likelihood-guided-regularization-for-transformers\\main\\pVisionTransformer_Trainer.py:786: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[VisionTransformerWithBBB(\n",
       "   (patch_embedding): BBBLinear(in_features=147, out_features=64, bias=True)\n",
       "   (encoder): ModuleList(\n",
       "     (0-1): 2 x TransformerEncoderLayerWithBBB(\n",
       "       (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "       (self_attn): MultiheadAttention(\n",
       "         (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "       )\n",
       "       (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): Sequential(\n",
       "         (0): BBBLinear(in_features=64, out_features=256, bias=True)\n",
       "         (1): GELU(approximate='none')\n",
       "         (2): Dropout(p=0.0, inplace=False)\n",
       "         (3): BBBLinear(in_features=256, out_features=64, bias=True)\n",
       "         (4): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (classification_head): Sequential(\n",
       "     (0): BBBLinear(in_features=64, out_features=32, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Dropout(p=0.0, inplace=False)\n",
       "     (3): BBBLinear(in_features=32, out_features=10, bias=True)\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e3897d-92ec-4a95-8a79-a91362dd6021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 - Accuracy: 86.13%\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cd70075-8fa2-4793-bc56-822b5aa51a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'mnist', 'train_samples': 8100, 'val_samples': 45900, 'test_samples': 6000, 'train_error': [0.29488151935331613], 'val_error': [0.4344054387675391], 'test_error': [0.4117753009001414], 'train_acc': [92.79012345679011], 'val_acc': [86.59477124183007], 'test_acc': [87.21666666666667], 'train_err': [7.209876543209887], 'val_err': [13.405228758169926], 'test_err': [12.783333333333331], 'num_parameters': 189908, 'ising_dropped': 36846}\n"
     ]
    }
   ],
   "source": [
    "stats = trainer.get_run_stats()\n",
    "print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
